<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://postyear.github.io</id>
    <title>Postyear</title>
    <updated>2021-03-31T04:29:47.003Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://postyear.github.io"/>
    <link rel="self" href="https://postyear.github.io/atom.xml"/>
    <subtitle>Postyear，记录一个普通程序员的生活</subtitle>
    <logo>https://postyear.github.io/images/avatar.png</logo>
    <icon>https://postyear.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Postyear</rights>
    <entry>
        <title type="html"><![CDATA[记录一个超难的机器学习项目：日志异常行为检测]]></title>
        <id>https://postyear.github.io/post/ji-lu-yi-ge-chao-nan-de-ji-qi-xue-xi-xiang-mu-ri-zhi-yi-chang-xing-wei-jian-ce/</id>
        <link href="https://postyear.github.io/post/ji-lu-yi-ge-chao-nan-de-ji-qi-xue-xi-xiang-mu-ri-zhi-yi-chang-xing-wei-jian-ce/">
        </link>
        <updated>2021-03-31T03:30:06.000Z</updated>
        <summary type="html"><![CDATA[<p><em>基于日志的异常行为检测，这是我最近接到的一个机器学习项目，目前已提交项目计划书，但还没有对方公司给的具体demo数据；和我之前做过的机器学习(入侵检测)任务不同，这一次有以下两个难点：</em></p>
]]></summary>
        <content type="html"><![CDATA[<p><em>基于日志的异常行为检测，这是我最近接到的一个机器学习项目，目前已提交项目计划书，但还没有对方公司给的具体demo数据；和我之前做过的机器学习(入侵检测)任务不同，这一次有以下两个难点：</em></p>
<!-- more -->
<ol>
<li>
<p>数据标注问题<br>
<strong>这次的数据完全无标注，标签都有自己生成。</strong></p>
<pre><code> 计划用Snorkel库解决这个问题。Snorkel是谷歌开源的机器学习库，用于半监督学习场景下的数据标记任务，需要开发人员设计标签函数。标签函数可以是基于规则（比如包含某些特定字段）、基于无监督分类器的，形式多样。
 暂时没有别的好的标注方法，只能先试一试；
 除此之外，还想到用公开的有标注的日志数据进行训练和测试。
</code></pre>
</li>
</ol>
<hr>
<!-- more -->
<ol start="2">
<li>
<p>构造特征<br>
<strong>这次是基于日志文本的，记录了每天不同用户的访问信息。</strong></p>
<pre><code> 自己总结了一些分别关于用户信息和事件信息的个性和共性特征，但我认为这仍然不够。需要结合文献再作扩展，特征构造的对象也没有确定。
  这个部分可以灵活调整，拿到数据再说。

  使用的检测模型，来看准备用单分类+序列挖掘；但后期也可以灵活调整。只不过项目书上这样写而已。这个项目，重点还是在于特征工程上。

 最后说下查到的相关资料，在27th ISSRE上有一篇关于日志检测文献*Experience Report: System Log Analysis for Anomaly Detection*，作者使用了两个公开的日志数据集，BGL和HDFS，自己设计了一套开源检测系统(https://github.com/logpai/loglizer)，共9种检测模型。具体做法：
 ●.日志收集和解析：日志解析的目的是将常量部分与变量部分分开，并形成一个成熟的日志事件
 ●.特征提取：该步骤的主要目的是从日志事件中提取有价值的特征，这些特征可以被输入异常检测模型。特征提取的输入是日志解析步骤中生成的日志事件，输出是特征矩阵（源论文中以0-1方式呈现，具体代码我还没看）。
 ●.异常检测模型：可以是有监督，也可以用无监督。在我这个项目中客户希望用无监督的方式去做。

 关于数据集的资料，参阅文献https://ieeexplore.ieee.org/document/4273008/。
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[略谈996]]></title>
        <id>https://postyear.github.io/post/lue-tan-996/</id>
        <link href="https://postyear.github.io/post/lue-tan-996/">
        </link>
        <updated>2021-03-12T13:56:57.000Z</updated>
        <content type="html"><![CDATA[<h1 id="正值两会召开全国政协委员李国华的一条建议刷屏了建议对996工作制进行监管">正值两会召开，全国政协委员李国华的一条建议刷屏了：建议对996工作制进行监管。</h1>
<pre><code>这说明996超额加班的情况已经很严重了。996不是互联网行业的专属，各行各业都有这个问题。

一位格力员工表示，目前已经连续工作20天中途没有休息过一天，每天早上八点钟或者晚上20点赶去开会，晚上21点或者早上9点才下班，非营销部的员工也必须销售产品……

一位快递小哥表示，  2个班次  1个月论调班  夜班12小时都是正常  一般都是14小时左右  这14小时你摸不了鱼  坐不了  手不能停  一直要弯腰用力  眼睛要死顶着快递面单……

还有从事土木行业的朋友表示，996真的是常态，在工地经常抢工时，不分白天昼夜，甚至不分周末周日……

正常来说，每人每天8小时就是正常的，8小时工作时间人手不够赶不上产量进度？多请人不就好了？还解决了就业问题，资本家那些就是抠，往死里压榨人，就算翻几倍，它也不可能没钱赚，多少的问题罢了

其实说996都是美化了“加班文化”，在制造业或者互联网行业待过的都知道，何止996，但是比996更恐怖的是领导鼓励加班，提倡奋斗，希望员工无私奉献，以加班为荣的精神pua,加班没有加班费，不允许提调休，年轻人多做事是好的，但是多少年轻人已经被折磨的没有生活乐趣，连好好坐下来吃个早餐都是奢望，难道为了工作就该放弃生活?虽然我年轻没经验，但我们愿意在规定时间想办法提高工作效率，而不是用生活和充电的时间做着重复却低效的工作。

这位李委员的话，得到了很多人的支持，共和国的未来不能依托于996。奋斗是必须的，但不应该是强制的，996≠奋斗。建议按照工作量安排休息，或者监管工作时长的同时还要监管工作量，并且专门成立一个劳动监管部门，不可以由其他任何部门兼职，不然增加了其他人员的工作量，他们也很难尽心尽力。每工作一段时间比如6个月必须更换监管单位，才能防止和企业联合一气，还要设置奖励，保证劳动人口依法休息就能有奖励，让这个部门能做到和企业斗智斗勇，防止企业想出什么骚操作来偷偷逼迫员工干活。</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2014年流行的聚类算法(Clustering by fast search and find of density peaksd)]]></title>
        <id>https://postyear.github.io/post/2014-nian-liu-xing-de-ju-lei-suan-fa-clustering-by-fast-search-and-find-of-density-peaksd/</id>
        <link href="https://postyear.github.io/post/2014-nian-liu-xing-de-ju-lei-suan-fa-clustering-by-fast-search-and-find-of-density-peaksd/">
        </link>
        <updated>2021-03-11T10:10:46.000Z</updated>
        <summary type="html"><![CDATA[<p>最近在学习聚类算法，很多聚类算法需要指定聚类个数，而聚类个数对聚类结果的影响非常大。</p>
<p>聚类个数一般作为kmeans等方法的前置参数。但也有一些聚类算法克服了缺点(变相转移)，比如：</p>
<p>1.Affinity Propagation（AP）聚类</p>
<p><strong>基本思想：</strong><br>
将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
无需指定K值，而且最终得到的聚类中心就是原数据中的点，收敛效果远胜于Kemans<br>
<strong>缺点：</strong><br>
计算复杂度大，时间长</p>
<p>2.meanshift<br>
<strong>基本思想</strong><br>
Mean shift 算法是基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。 MeanShift，顾名思义，由Mean（均值）和shift（偏移）组成。也就是有一个点x，周围有很多点xi，我们计算点x移动到每个点所需要的偏移量之和，求平均，就得到平均偏移量。该偏移量包含大小和方向 ，方向就是周围分布密集的方向。然后点x往平均偏移量方向移动，再以此为新起点，不断迭代直到满足一定条件结束。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
几乎是没有参数的，带宽可以用sklearn自带的函数估计出来。</p>
<p><strong>缺点</strong><br>
聚类性能不是那么强</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近在学习聚类算法，很多聚类算法需要指定聚类个数，而聚类个数对聚类结果的影响非常大。</p>
<p>聚类个数一般作为kmeans等方法的前置参数。但也有一些聚类算法克服了缺点(变相转移)，比如：</p>
<p>1.Affinity Propagation（AP）聚类</p>
<p><strong>基本思想：</strong><br>
将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
无需指定K值，而且最终得到的聚类中心就是原数据中的点，收敛效果远胜于Kemans<br>
<strong>缺点：</strong><br>
计算复杂度大，时间长</p>
<p>2.meanshift<br>
<strong>基本思想</strong><br>
Mean shift 算法是基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。 MeanShift，顾名思义，由Mean（均值）和shift（偏移）组成。也就是有一个点x，周围有很多点xi，我们计算点x移动到每个点所需要的偏移量之和，求平均，就得到平均偏移量。该偏移量包含大小和方向 ，方向就是周围分布密集的方向。然后点x往平均偏移量方向移动，再以此为新起点，不断迭代直到满足一定条件结束。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
几乎是没有参数的，带宽可以用sklearn自带的函数估计出来。</p>
<p><strong>缺点</strong><br>
聚类性能不是那么强</p>
<!-- more -->
<p>3.密度峰值</p>
<p><strong>核心思想</strong><br>
对聚类中心的刻画上，而且认为聚类中心同时具有以下两种特点：<br>
1）本身的密度大，即它被密度均不超过它的邻居包围<br>
2）与其他密度更大的数据点之间的“距离”相对更大</p>
<p>这个算法是14年刚出现的，从GitHub的搜索结果来看也很流行，不同语言的都有。简单来说分四步：<br>
第1步，求每个点的密度rho。点的密度就是，以点为中心，以dc为半径，画一个小圆圈，数数里面几个点，圆圈中点的个数就是点的密度。（还可以用高斯核密度求点的密度，求出来的密度是连续型的）<br>
第2步，计算每个点的delta。假设有一个点x，求比点x的密度大的且距离点x最近的那个点y，那么点x与点y之间的距离，就是点x的delta，就这样遍历所有点，把每个点的delta都求出来（注意，delta是距离，谁和谁的距离？x和y的距离，y是谁？y就是比x密度大，且距离x最近的那个点，要满足两个条件，密度比x大且距离最近）<br>
第3步，每个点的密度rho和delta都求出来了，以rho为横坐标，delta为纵坐标，画个二维图，图中右上角的那几个点就是聚类中心，也就是rho和delta都很大的那几个点。（为什么？因为聚类中心有个特点，密度很大，且与密度比它大的点的距离也很大）<br>
第4步，找到聚类中心了，就可以扩展聚类簇了，按照rho从大到小的顺序进行聚类扩展。<br>
转自知乎，链接：https://www.zhihu.com/question/65977238/answer/923739282</p>
<p>密度峰值算法需要输入参数t，为百分数的整数部分，如2%，用于确定截断距离Dc。<strong>作者建议取1%-2%</strong>。 one can choose d c so that the average number of neighbors is around 1 to 2% of the total number of points in the data set.(论文原话)</p>
<p>可以参阅文章：https://www.cnblogs.com/nolonely/p/6964852.html</p>
<p>python实现代码：<br>
1.https://github.com/Larry213021/Clustering-by-fast-search-and-find-of-density-peaks/blob/main/classification/CFSDP(%E4%BB%A5t%E7%82%BA%E5%9F%BA%E6%BA%96).py<br>
2.https://github.com/Kingzy-Chen/DensityPeakCluster<br>
3.https://github.com/cwehmeyer/pydpc (py库)</p>
<p><strong>优点：</strong><br>
无需k值，性能过得去<br>
<strong>缺点：</strong><br>
似乎在高维数据上表现不佳；</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[入侵检测中使用的数据增强]]></title>
        <id>https://postyear.github.io/post/ru-qin-jian-ce-zhong-shi-yong-de-shu-ju-zeng-qiang/</id>
        <link href="https://postyear.github.io/post/ru-qin-jian-ce-zhong-shi-yong-de-shu-ju-zeng-qiang/">
        </link>
        <updated>2021-03-10T12:51:22.000Z</updated>
        <summary type="html"><![CDATA[<p>在入侵检测中，一般特征选择用得非常多，数据增强技术使用比较少。但也有。</p>
<p>数据增强在神经网络中更常见，主要用来防止过拟合，用于dataset较小的时候。</p>
<p>之前对神经网络有过了解的人都知道，虽然一个两层网络在理论上可以拟合所有的分布，但是并不容易学习得到。因此在实际中，我们通常会增加神经网络的深度和广度，从而让神经网络的学习能力增强，便于拟合训练数据的分布情况。在卷积神经网络中，有人实验得到，深度比广度更重要。</p>
<p>增强后的数据维度没有发生变化。今天记录的就是这样一种有监督的数据增强方法，计算特征的边缘密度比(marginal density ratio)替换原特征，可以加速训练。最初是国外一篇论文提出的，被用于基于SVM的入侵检测论文中，尽管作者表明可以替换为任意分类器，但应该最适合SVM。步骤如下：</p>
<figure data-type="image" tabindex="1"><img src="https://postyear.github.io/post-images/1615381362646.png" alt="" loading="lazy"></figure>
<p>作者是用R语言实现的，R包不太会用，后来就没有用。链接见下方：</p>
]]></summary>
        <content type="html"><![CDATA[<p>在入侵检测中，一般特征选择用得非常多，数据增强技术使用比较少。但也有。</p>
<p>数据增强在神经网络中更常见，主要用来防止过拟合，用于dataset较小的时候。</p>
<p>之前对神经网络有过了解的人都知道，虽然一个两层网络在理论上可以拟合所有的分布，但是并不容易学习得到。因此在实际中，我们通常会增加神经网络的深度和广度，从而让神经网络的学习能力增强，便于拟合训练数据的分布情况。在卷积神经网络中，有人实验得到，深度比广度更重要。</p>
<p>增强后的数据维度没有发生变化。今天记录的就是这样一种有监督的数据增强方法，计算特征的边缘密度比(marginal density ratio)替换原特征，可以加速训练。最初是国外一篇论文提出的，被用于基于SVM的入侵检测论文中，尽管作者表明可以替换为任意分类器，但应该最适合SVM。步骤如下：</p>
<figure data-type="image" tabindex="1"><img src="https://postyear.github.io/post-images/1615381362646.png" alt="" loading="lazy"></figure>
<p>作者是用R语言实现的，R包不太会用，后来就没有用。链接见下方：</p>
<!-- more -->
<p><strong>Paper</strong>：A novel approach to intrusion detection using<br>
SVM ensemble with feature augmentation<br>
<strong>Github</strong>：https://github.com/ShanLu92/FeaAug/blob/4df07a5c099b8e4b234ba3156208c2b671c127b2/R/LMDRT.R&gt;</p>
<!-- more -->
<p>还有这篇论文Augboost(Gradient Boosting Enhanced with Step-Wise Feature Augmentation)也是提出了一种新的数据增强技术，无监督，百度可以查到介绍，<br>
https://github.com/augboost-anon/augboost</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[韩山]]></title>
        <id>https://postyear.github.io/post/han-shan-very-hot/</id>
        <link href="https://postyear.github.io/post/han-shan-very-hot/">
        </link>
        <updated>2021-02-21T12:21:34.000Z</updated>
        <content type="html"><![CDATA[<p>韩山，比想象中好玩！就是今天天太热了！<br>
<img src="https://postyear.github.io/post-images/1613910525338.jpg" alt="" loading="lazy"><br>
<img src="https://postyear.github.io/post-images/1613910705228.jpg" alt="" loading="lazy"><br>
<img src="https://postyear.github.io/post-images/1613910716704.jpg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[第一天]]></title>
        <id>https://postyear.github.io/post/di-yi-tian/</id>
        <link href="https://postyear.github.io/post/di-yi-tian/">
        </link>
        <updated>2021-02-18T13:11:28.000Z</updated>
        <content type="html"><![CDATA[<p>开通博客的第一天</p>
]]></content>
    </entry>
</feed>