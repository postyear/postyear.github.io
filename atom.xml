<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://postyear.github.io</id>
    <title>Postyear</title>
    <updated>2021-07-26T14:28:51.453Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://postyear.github.io"/>
    <link rel="self" href="https://postyear.github.io/atom.xml"/>
    <subtitle>Postyear，记录一个普通程序员的生活</subtitle>
    <logo>https://postyear.github.io/images/avatar.png</logo>
    <icon>https://postyear.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Postyear</rights>
    <entry>
        <title type="html"><![CDATA[Spring Boot中的分层：controller，service，dao，mapper]]></title>
        <id>https://postyear.github.io/post/spring-boot-zhong-de-fen-ceng-controllerservicedaomapper/</id>
        <link href="https://postyear.github.io/post/spring-boot-zhong-de-fen-ceng-controllerservicedaomapper/">
        </link>
        <updated>2021-07-26T14:04:26.000Z</updated>
        <summary type="html"><![CDATA[<p>最近来到了公司实习，也接触到了项目业务逻辑。特别是企业项目，文件很多，理清业务逻辑很有必要。</p>
<p>项目使用Sping CLoud开发，而Spring Cloud基于Spring Boot设计。使用Spring Boot框架开发，必须设计controller，service，dao，mapper等层。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近来到了公司实习，也接触到了项目业务逻辑。特别是企业项目，文件很多，理清业务逻辑很有必要。</p>
<p>项目使用Sping CLoud开发，而Spring Cloud基于Spring Boot设计。使用Spring Boot框架开发，必须设计controller，service，dao，mapper等层。</p>
<!-- more -->
<p>1.Dao(Data Access Object):数据存储对象<br>
DAO = Data Access Object = 数据存取对象. 不管是什么框架，我们很多时候都会与数据库进行交互。如果遇到一个场景我们都要去写SQL语句，那么我们的代码就会很冗余。所以，我们就想到了把数据库封装一下，让我们的数据库的交道看起来像和一个对象打交道，这个对象通常就是DAO。当我们操作这个对象的时候，这个对象会自动产生SQL语句来和数据库进行交互，我们就只需要使用DAO就行了。</p>
<pre><code>通常我们在DAO层里面写接口，里面有与数据打交道的方法。SQL语句通常写在mapper文件里面的。

优点：结构清晰，Dao层的数据源配置以及相关的有关数据库连接的参数都在Spring配置文件中进行配置。
</code></pre>
<p>2.Service：服务（Biz）<br>
服务是一个相对独立的功能模块，主要负责业务逻辑应用设计。首先也要设计接口，然后再设计其实现该接口的类。这样我们就可以在应用中调用service接口进行业务处理。service层业务实现，具体调用到已经定义的DAO的接口，封装service层的业务逻辑有利于通用的业务逻辑的独立性和重复利用性 。</p>
<pre><code>如果把Dao层当作积木，则Service层则是对积木的搭建。
</code></pre>
<p>3.Controller：控制器<br>
主要负责具体业务模块流程的控制，此层要调用到Service层的接口去控制业务流程，控制的配置同样在Spring配置文件中配置。针对不同的业务流程有不同的控制器。在设计的过程可以设计出重复利用的子单元流程模块。<br>
在企业项目中，一般和前端路由相关，处理访问请求。</p>
<p>4.Model：模型<br>
模型就是指视图的数据Model，模型，通常来讲，我们会把模型和另一个东西放在一起来说：View，视图。</p>
<pre><code>模型通常认为是视图的内核，何谓之视图？我们正在与之交互的网站的界面就是视图，而模型是指他的内核：数据。

    将Model和View的概念拆分开来，有助于我们关注不同的方面，也可以更有效的分工。有些工程师更关注于内核也就是模型，通常来说，他们被称之为后端工程师。有些工程师更关注于用户界面的交互和展示，通常来说，他们被称之为前端工程师。
</code></pre>
<p>简而言之，<strong>Controller接收参数，Service处理业务，Dao(mapper)对数据库进行操作~</strong><br>
划分层次就是为了分工明细(阅读和修改都方便)，降低了维护难度，代码清晰。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据库的隔离级别]]></title>
        <id>https://postyear.github.io/post/shu-ju-ku-de-ge-chi-ji-bie/</id>
        <link href="https://postyear.github.io/post/shu-ju-ku-de-ge-chi-ji-bie/">
        </link>
        <updated>2021-04-22T12:10:21.000Z</updated>
        <content type="html"><![CDATA[<p>原文转自https://www.cnblogs.com/ubuntu1/p/8999403.html。</p>
<p>数据库事务的隔离级别有4种，由低到高分别为Read uncommitted 、Read committed 、Repeatable read 、Serializable 。而且，在事务的并发操作中可能会出现脏读，不可重复读，幻读。下面通过事例一一阐述它们的概念与联系。</p>
<p><strong>1.Read uncommitted</strong></p>
<p>读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。</p>
<p>事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。</p>
<p>分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。</p>
<p><strong>2.Read committed</strong></p>
<p>读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。</p>
<p>事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的…</p>
<p>分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。</p>
<p><strong>3.Repeatable read</strong></p>
<p>重复读，就是在开始读取数据（事务开启）时，不再允许修改操作</p>
<p>事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。</p>
<p>分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。</p>
<p><strong>4.Serializable</strong></p>
<p>Serializable  序列化，是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[新版Navicat操作MySQL]]></title>
        <id>https://postyear.github.io/post/xin-ban-navicat-cao-zuo-mysql/</id>
        <link href="https://postyear.github.io/post/xin-ban-navicat-cao-zuo-mysql/">
        </link>
        <updated>2021-04-21T08:47:39.000Z</updated>
        <content type="html"><![CDATA[<p>之前Mysql命令行用得多一些，但还是很不方便的。最近开始使用Navicat。使用的是比较新的版本了，Navicat Premium 15. 这个版本改动挺大的，一些操作都不一样。</p>
<p>软件参照链接进行注册、使用。https://my.oschina.net/SoberLee/blog/4746541?hmsr=kaifa_aladdin。注册机的名称叫做：Navicat Keygen Patch v5.6.0 DFoX.exe。</p>
<p>假设有这样一张表，记录了学生的成绩、姓名、年龄等信息：<br>
<img src="https://postyear.github.io/post-images/1618995267923.png" alt="" loading="lazy"><br>
对表的相关操作都转移到 <em><strong>右键-设计表</strong></em> 里面去了。比如对字段的插入、删除等。或者sql语句：<br>
alter table <code>test</code> add column colum_name varchar(255) not null</p>
<p>现在我需要找出根据成绩(scores)字段降序(加上 DESC)排序得到的学生，那么应该使用的SQL语句是：<br>
SELECT student, age,scores FROM <code>test</code> order by scores DESC</p>
<p>如果指定位置进行查找，并对返回记录的数量做出限制，在上一条语句结尾加上：<br>
SELECT student, age,scores FROM <code>test</code> order by scores DESC LIMIT 2,3</p>
<p>Limit设定从第二行开始，返回三条记录。参数X，Y 就表示查询的起始位置和返回记录数，X可省略。<br>
但Mysql中不支持select top n的语法，TOP也没有关键字提示。</p>
<p>接下来我们使用 GROUP BY 语句 将数据表按年龄进行分组，并统计每种年龄有多少条记录：<br>
SELECT age, COUNT(<em>)  FROM <code>test</code> GROUP BY age<br>
选择对象需和group by的对象一致，结果如下：<br>
<img src="https://postyear.github.io/post-images/1618996647485.png" alt="" loading="lazy"><br>
如果使用SELECT age</em>COUNT(*)  FROM <code>test</code> GROUP BY age 那么就得到各自年龄的总和：<br>
<img src="https://postyear.github.io/post-images/1618997137378.png" alt="" loading="lazy"></p>
<p>总之mysql的操作非常重要，更是面试的重点，要加强学习。最后补充下Mysql三大范式：</p>
<p>1.第一范式（1NF）：要求数据库表的每一列都是不可分割的基本数据项(原子字段)。强调列的原子性，表中某一属性不能拥有几个值。</p>
<p>2.第二范式（2NF）：第二范式(确保表中的每列都和主键相关)第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。<strong>第二范式要求数据表每一个实例或者行必须被唯一标识。除满足第一范式外还有两个条件，一是表必须有一个主键；二是没有包含在主键中的列必须完全依赖于主键。</strong></p>
<p>3.第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。<strong>即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记录一个超难的机器学习项目：日志异常行为检测]]></title>
        <id>https://postyear.github.io/post/ji-lu-yi-ge-chao-nan-de-ji-qi-xue-xi-xiang-mu-ri-zhi-yi-chang-xing-wei-jian-ce/</id>
        <link href="https://postyear.github.io/post/ji-lu-yi-ge-chao-nan-de-ji-qi-xue-xi-xiang-mu-ri-zhi-yi-chang-xing-wei-jian-ce/">
        </link>
        <updated>2021-03-31T03:30:06.000Z</updated>
        <summary type="html"><![CDATA[<p><em>基于日志的异常行为检测，这是我最近接到的一个机器学习项目，目前已提交项目计划书，但还没有对方公司给的具体demo数据；和我之前做过的机器学习(入侵检测)任务不同，这一次有以下两个难点：</em></p>
]]></summary>
        <content type="html"><![CDATA[<p><em>基于日志的异常行为检测，这是我最近接到的一个机器学习项目，目前已提交项目计划书，但还没有对方公司给的具体demo数据；和我之前做过的机器学习(入侵检测)任务不同，这一次有以下两个难点：</em></p>
<!-- more -->
<ol>
<li>
<p>数据标注问题<br>
<strong>这次的数据完全无标注，标签都有自己生成。</strong></p>
<pre><code> 计划用Snorkel库解决这个问题。Snorkel是谷歌开源的机器学习库，用于半监督学习场景下的数据标记任务，需要开发人员设计标签函数。标签函数可以是基于规则（比如包含某些特定字段）、基于无监督分类器的，形式多样。
 暂时没有别的好的标注方法，只能先试一试；
 除此之外，还想到用公开的有标注的日志数据进行训练和测试。
</code></pre>
</li>
</ol>
<hr>
<!-- more -->
<ol start="2">
<li>
<p>构造特征<br>
<strong>这次是基于日志文本的，记录了每天不同用户的访问信息。</strong></p>
<pre><code> 自己总结了一些分别关于用户信息和事件信息的个性和共性特征，但我认为这仍然不够。需要结合文献再作扩展，特征构造的对象也没有确定。
  这个部分可以灵活调整，拿到数据再说。

  使用的检测模型，来看准备用单分类+序列挖掘；但后期也可以灵活调整。只不过项目书上这样写而已。这个项目，重点还是在于特征工程上。

 最后说下查到的相关资料，在27th ISSRE上有一篇关于日志检测文献*Experience Report: System Log Analysis for Anomaly Detection*，作者使用了两个公开的日志数据集，BGL和HDFS，自己设计了一套开源检测系统(https://github.com/logpai/loglizer)，共9种检测模型。具体做法：
 ●.日志收集和解析：日志解析的目的是将常量部分与变量部分分开，并形成一个成熟的日志事件
 ●.特征提取：该步骤的主要目的是从日志事件中提取有价值的特征，这些特征可以被输入异常检测模型。特征提取的输入是日志解析步骤中生成的日志事件，输出是特征矩阵（源论文中以0-1方式呈现，具体代码我还没看）。
 ●.异常检测模型：可以是有监督，也可以用无监督。在我这个项目中客户希望用无监督的方式去做。

 关于数据集的资料，参阅文献https://ieeexplore.ieee.org/document/4273008/。
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[略谈996]]></title>
        <id>https://postyear.github.io/post/lue-tan-996/</id>
        <link href="https://postyear.github.io/post/lue-tan-996/">
        </link>
        <updated>2021-03-12T13:56:57.000Z</updated>
        <content type="html"><![CDATA[<h1 id="正值两会召开全国政协委员李国华的一条建议刷屏了建议对996工作制进行监管">正值两会召开，全国政协委员李国华的一条建议刷屏了：建议对996工作制进行监管。</h1>
<p>这说明996超额加班的情况已经很严重了。996不是互联网行业的专属，各行各业都有这个问题。<br>
一位格力员工表示，目前已经连续工作20天中途没有休息过一天，每天早上八点钟或者晚上20点赶去开会，晚上21点或者早上9点才下班，非营销部的员工也必须销售产品……<br>
一位快递小哥表示， 2个班次 1个月论调班 夜班12小时都是正常 一般都是14小时左右 这14小时你摸不了鱼 坐不了 手不能停 一直要弯腰用力 眼睛要死顶着快递面单……<br>
还有从事土木行业的朋友表示，996真的是常态，在工地经常抢工时，不分白天昼夜，甚至不分周末周日……</p>
<p>正常来说，每人每天8小时就是正常的，8小时工作时间人手不够赶不上产量进度？多请人不就好了？还解决了就业问题，资本家那些就是抠，往死里压榨人，就算翻几倍，它也不可能没钱赚，多少的问题罢了 其实说996都是美化了“加班文化”，在制造业或者互联网行业待过的都知道，何止996，但是比996更恐怖的是领导鼓励加班，提倡奋斗，希望员工无私奉献，以加班为荣的精神pua,加班没有加班费，不允许提调休，年轻人多做事是好的，但是多少年轻人已经被折磨的没有生活乐趣，连好好坐下来吃个早餐都是奢望，难道为了工作就该放弃生活?虽然我年轻没经验，但我们愿意在规定时间想办法提高工作效率，而不是用生活和充电的时间做着重复却低效的工作。 这位李委员的话，得到了很多人的支持，共和国的未来不能依托于996。奋斗是必须的，但不应该是强制的，996≠奋斗。建议按照工作量安排休息，或者监管工作时长的同时还要监管工作量，并且专门成立一个劳动监管部门，不可以由其他任何部门兼职，不然增加了其他人员的工作量，他们也很难尽心尽力。每工作一段时间比如6个月必须更换监管单位，才能防止和企业联合一气，还要设置奖励，保证劳动人口依法休息就能有奖励，让这个部门能做到和企业斗智斗勇，防止企业想出什么骚操作来偷偷逼迫员工干活。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2014年流行的聚类算法(Clustering by fast search and find of density peaksd)]]></title>
        <id>https://postyear.github.io/post/2014-nian-liu-xing-de-ju-lei-suan-fa-clustering-by-fast-search-and-find-of-density-peaksd/</id>
        <link href="https://postyear.github.io/post/2014-nian-liu-xing-de-ju-lei-suan-fa-clustering-by-fast-search-and-find-of-density-peaksd/">
        </link>
        <updated>2021-03-11T10:10:46.000Z</updated>
        <summary type="html"><![CDATA[<p>最近在学习聚类算法，很多聚类算法需要指定聚类个数，而聚类个数对聚类结果的影响非常大。</p>
<p>聚类个数一般作为kmeans等方法的前置参数。但也有一些聚类算法克服了缺点(变相转移)，比如：</p>
<p>1.Affinity Propagation（AP）聚类</p>
<p><strong>基本思想：</strong><br>
将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
无需指定K值，而且最终得到的聚类中心就是原数据中的点，收敛效果远胜于Kemans<br>
<strong>缺点：</strong><br>
计算复杂度大，时间长</p>
<p>2.meanshift<br>
<strong>基本思想</strong><br>
Mean shift 算法是基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。 MeanShift，顾名思义，由Mean（均值）和shift（偏移）组成。也就是有一个点x，周围有很多点xi，我们计算点x移动到每个点所需要的偏移量之和，求平均，就得到平均偏移量。该偏移量包含大小和方向 ，方向就是周围分布密集的方向。然后点x往平均偏移量方向移动，再以此为新起点，不断迭代直到满足一定条件结束。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
几乎是没有参数的，带宽可以用sklearn自带的函数估计出来。</p>
<p><strong>缺点</strong><br>
聚类性能不是那么强</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近在学习聚类算法，很多聚类算法需要指定聚类个数，而聚类个数对聚类结果的影响非常大。</p>
<p>聚类个数一般作为kmeans等方法的前置参数。但也有一些聚类算法克服了缺点(变相转移)，比如：</p>
<p>1.Affinity Propagation（AP）聚类</p>
<p><strong>基本思想：</strong><br>
将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
无需指定K值，而且最终得到的聚类中心就是原数据中的点，收敛效果远胜于Kemans<br>
<strong>缺点：</strong><br>
计算复杂度大，时间长</p>
<p>2.meanshift<br>
<strong>基本思想</strong><br>
Mean shift 算法是基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。 MeanShift，顾名思义，由Mean（均值）和shift（偏移）组成。也就是有一个点x，周围有很多点xi，我们计算点x移动到每个点所需要的偏移量之和，求平均，就得到平均偏移量。该偏移量包含大小和方向 ，方向就是周围分布密集的方向。然后点x往平均偏移量方向移动，再以此为新起点，不断迭代直到满足一定条件结束。</p>
<p>已被sklearn.cluster收录</p>
<p><strong>优点：</strong><br>
几乎是没有参数的，带宽可以用sklearn自带的函数估计出来。</p>
<p><strong>缺点</strong><br>
聚类性能不是那么强</p>
<!-- more -->
<p>3.密度峰值</p>
<p><strong>核心思想</strong><br>
对聚类中心的刻画上，而且认为聚类中心同时具有以下两种特点：<br>
1）本身的密度大，即它被密度均不超过它的邻居包围<br>
2）与其他密度更大的数据点之间的“距离”相对更大</p>
<p>这个算法是14年刚出现的，从GitHub的搜索结果来看也很流行，不同语言的都有。简单来说分四步：<br>
第1步，求每个点的密度rho。点的密度就是，以点为中心，以dc为半径，画一个小圆圈，数数里面几个点，圆圈中点的个数就是点的密度。（还可以用高斯核密度求点的密度，求出来的密度是连续型的）<br>
第2步，计算每个点的delta。假设有一个点x，求比点x的密度大的且距离点x最近的那个点y，那么点x与点y之间的距离，就是点x的delta，就这样遍历所有点，把每个点的delta都求出来（注意，delta是距离，谁和谁的距离？x和y的距离，y是谁？y就是比x密度大，且距离x最近的那个点，要满足两个条件，密度比x大且距离最近）<br>
第3步，每个点的密度rho和delta都求出来了，以rho为横坐标，delta为纵坐标，画个二维图，图中右上角的那几个点就是聚类中心，也就是rho和delta都很大的那几个点。（为什么？因为聚类中心有个特点，密度很大，且与密度比它大的点的距离也很大）<br>
第4步，找到聚类中心了，就可以扩展聚类簇了，按照rho从大到小的顺序进行聚类扩展。<br>
转自知乎，链接：https://www.zhihu.com/question/65977238/answer/923739282</p>
<p>密度峰值算法需要输入参数t，为百分数的整数部分，如2%，用于确定截断距离Dc。<strong>作者建议取1%-2%</strong>。 one can choose d c so that the average number of neighbors is around 1 to 2% of the total number of points in the data set.(论文原话)</p>
<p>可以参阅文章：https://www.cnblogs.com/nolonely/p/6964852.html</p>
<p>python实现代码：<br>
1.https://github.com/Larry213021/Clustering-by-fast-search-and-find-of-density-peaks/blob/main/classification/CFSDP(%E4%BB%A5t%E7%82%BA%E5%9F%BA%E6%BA%96).py<br>
2.https://github.com/Kingzy-Chen/DensityPeakCluster<br>
3.https://github.com/cwehmeyer/pydpc (py库)</p>
<p><strong>优点：</strong><br>
无需k值，性能过得去<br>
<strong>缺点：</strong><br>
似乎在高维数据上表现不佳；</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[入侵检测中使用的数据增强]]></title>
        <id>https://postyear.github.io/post/ru-qin-jian-ce-zhong-shi-yong-de-shu-ju-zeng-qiang/</id>
        <link href="https://postyear.github.io/post/ru-qin-jian-ce-zhong-shi-yong-de-shu-ju-zeng-qiang/">
        </link>
        <updated>2021-03-10T12:51:22.000Z</updated>
        <summary type="html"><![CDATA[<p>在入侵检测中，一般特征选择用得非常多，数据增强技术使用比较少。但也有。</p>
<p>数据增强在神经网络中更常见，主要用来防止过拟合，用于dataset较小的时候。</p>
<p>之前对神经网络有过了解的人都知道，虽然一个两层网络在理论上可以拟合所有的分布，但是并不容易学习得到。因此在实际中，我们通常会增加神经网络的深度和广度，从而让神经网络的学习能力增强，便于拟合训练数据的分布情况。在卷积神经网络中，有人实验得到，深度比广度更重要。</p>
<p>增强后的数据维度没有发生变化。今天记录的就是这样一种有监督的数据增强方法，计算特征的边缘密度比(marginal density ratio)替换原特征，可以加速训练。最初是国外一篇论文提出的，被用于基于SVM的入侵检测论文中，尽管作者表明可以替换为任意分类器，但应该最适合SVM。步骤如下：</p>
<figure data-type="image" tabindex="1"><img src="https://postyear.github.io/post-images/1615381362646.png" alt="" loading="lazy"></figure>
<p>作者是用R语言实现的，R包不太会用，后来就没有用。链接见下方：</p>
]]></summary>
        <content type="html"><![CDATA[<p>在入侵检测中，一般特征选择用得非常多，数据增强技术使用比较少。但也有。</p>
<p>数据增强在神经网络中更常见，主要用来防止过拟合，用于dataset较小的时候。</p>
<p>之前对神经网络有过了解的人都知道，虽然一个两层网络在理论上可以拟合所有的分布，但是并不容易学习得到。因此在实际中，我们通常会增加神经网络的深度和广度，从而让神经网络的学习能力增强，便于拟合训练数据的分布情况。在卷积神经网络中，有人实验得到，深度比广度更重要。</p>
<p>增强后的数据维度没有发生变化。今天记录的就是这样一种有监督的数据增强方法，计算特征的边缘密度比(marginal density ratio)替换原特征，可以加速训练。最初是国外一篇论文提出的，被用于基于SVM的入侵检测论文中，尽管作者表明可以替换为任意分类器，但应该最适合SVM。步骤如下：</p>
<figure data-type="image" tabindex="1"><img src="https://postyear.github.io/post-images/1615381362646.png" alt="" loading="lazy"></figure>
<p>作者是用R语言实现的，R包不太会用，后来就没有用。链接见下方：</p>
<!-- more -->
<p><strong>Paper</strong>：A novel approach to intrusion detection using<br>
SVM ensemble with feature augmentation<br>
<strong>Github</strong>：https://github.com/ShanLu92/FeaAug/blob/4df07a5c099b8e4b234ba3156208c2b671c127b2/R/LMDRT.R&gt;</p>
<!-- more -->
<p>还有这篇论文Augboost(Gradient Boosting Enhanced with Step-Wise Feature Augmentation)也是提出了一种新的数据增强技术，无监督，百度可以查到介绍，<br>
https://github.com/augboost-anon/augboost</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[韩山]]></title>
        <id>https://postyear.github.io/post/han-shan-very-hot/</id>
        <link href="https://postyear.github.io/post/han-shan-very-hot/">
        </link>
        <updated>2021-02-21T12:21:34.000Z</updated>
        <content type="html"><![CDATA[<p>韩山，比想象中好玩！就是今天天太热了！<br>
<img src="https://postyear.github.io/post-images/1613910525338.jpg" alt="" loading="lazy"><br>
<img src="https://postyear.github.io/post-images/1613910705228.jpg" alt="" loading="lazy"><br>
<img src="https://postyear.github.io/post-images/1613910716704.jpg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[第一天]]></title>
        <id>https://postyear.github.io/post/di-yi-tian/</id>
        <link href="https://postyear.github.io/post/di-yi-tian/">
        </link>
        <updated>2021-02-18T13:11:28.000Z</updated>
        <content type="html"><![CDATA[<p>开通博客的第一天</p>
]]></content>
    </entry>
</feed>